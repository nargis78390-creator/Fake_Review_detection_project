{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nargis78390-creator/Fake_Review_detection_project/blob/main/MinorProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMn8ga2N8K5A"
      },
      "source": [
        "importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_x_RHk5MLTR4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC5O7HjPCr9v",
        "outputId": "ac41831c-d859-425e-987b-d97a018d3ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2POU-TSMalr"
      },
      "source": [
        "to connect google drive and google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "DUoX0NnbKeu7",
        "outputId": "4419751e-024f-4c07-b881-6b5a2c117171"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJpyQgUdxlhI"
      },
      "source": [
        "dataset is loaded from a CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrGmjNwBLvFa"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/fake reviews dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-IDmgcJ_C_r"
      },
      "outputs": [],
      "source": [
        "length=len(df)\n",
        "print(length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew_iukFlOekr"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Check the first 100 rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajzZRsZvL7TM"
      },
      "outputs": [],
      "source": [
        "df.head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWdVQzbUOmMa"
      },
      "source": [
        "check the last five rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T5EJI9aNdD7"
      },
      "outputs": [],
      "source": [
        "df.tail(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6q0RsaMOB6X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFnDEHMLOxbH"
      },
      "source": [
        "Check for any missing values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcazfLA9PD-M"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLCShsFTm88B"
      },
      "source": [
        "assess the structure and quality of your DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuaG8Tlrm7rd"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JER3uZmy2TFK"
      },
      "source": [
        " for a quick statistical summary of numerical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPCHb8YLnPLf"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXDsz7gYol-N"
      },
      "outputs": [],
      "source": [
        "df['rating'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypAfeaUL2m6C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a__Msem2oKC"
      },
      "source": [
        "removing punctuation and stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oXpgGHIRTHw"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    nopunc = [w for w in text if w not in string.punctuation]\n",
        "    nopunc = ''.join(nopunc)\n",
        "    return  ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx5q7V9-RYRV"
      },
      "outputs": [],
      "source": [
        "df['text_'][0], clean_text(df['text_'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOc8lSjGehjx"
      },
      "outputs": [],
      "source": [
        "df['text_'].head().apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFO5RrhBet-J"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7e5-Zi-fZgz"
      },
      "outputs": [],
      "source": [
        "df['text_'] = df['text_'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJcQtai5pp2r"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    return ' '.join([word for word in word_tokenize(text) if word not in stopwords.words('english') and not word.isdigit() and word not in string.punctuation])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KaHCHZgJhNq"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zg3SIOZapuBS"
      },
      "outputs": [],
      "source": [
        "preprocess(df['text_'][4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhT6LoUrRfzV"
      },
      "outputs": [],
      "source": [
        "df['text_'][:10000] = df['text_'][:10000].apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y77AW1cFryAe"
      },
      "outputs": [],
      "source": [
        "df['text_'][10001:20000] = df['text_'][10001:20000].apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDazRjjdr_SW"
      },
      "outputs": [],
      "source": [
        "df['text_'][20001:30000] = df['text_'][20001:30000].apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cejhi3oYsEOU"
      },
      "outputs": [],
      "source": [
        "df['text_'][30001:40000] = df['text_'][30001:40000].apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkb3Q7KKsEVR"
      },
      "outputs": [],
      "source": [
        "df['text_'][40001:40432] = df['text_'][40001:40432].apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw6cIkkgsEkK"
      },
      "outputs": [],
      "source": [
        "df['text_'] = df['text_'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsGKKqmmsPoJ"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "def stem_words(text):\n",
        "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "df['text_'] = df['text_'].apply(lambda x: stem_words(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCR5DEkcRmoU"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words(text):\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "df[\"text_\"] = df[\"text_\"].apply(lambda text: lemmatize_words(text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYmP30YtTr3Z"
      },
      "outputs": [],
      "source": [
        "df['text_'].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYnbRrz8Txp0"
      },
      "outputs": [],
      "source": [
        "df.to_csv('Preprocessed Fake Reviews Detection Dataset.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIGikmPhUD4o"
      },
      "outputs": [],
      "source": [
        "df.columns = df.columns.str.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOi2sy6-PjVa"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2WE3S8aR2KB"
      },
      "source": [
        "Apply cleaning function to your text column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaYOAlHrLve3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "# Define the clean_text function to process the text\n",
        "def clean_text(text):\n",
        "    # Remove punctuation\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove digits\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = text.strip()\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLIObKt9RRXT"
      },
      "outputs": [],
      "source": [
        "df['cleaned_reviews'] = df['text_'].apply(clean_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FAqBlvwOxV4"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=['category','rating','text_'],inplace=True,errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2e08f-hO0OK"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck5PqT2z9lva"
      },
      "outputs": [],
      "source": [
        "# draw wordcloud (for fake review)\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "text = ' '.join(word for word in df['cleaned_reviews'][df['label'] == 'CG'].astype(str))\n",
        "wordcloud = WordCloud(width=1000, height=600,max_font_size = 100)\n",
        "plt.figure(figsize=(15,12))\n",
        "plt.imshow(wordcloud.generate(text), interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1zNwGan-gGp"
      },
      "outputs": [],
      "source": [
        "# draw wordcloud (for original or real review)\n",
        "\n",
        "text = ' '.join(word for word in df['cleaned_reviews'][df['label'] == 'OR'].astype(str))\n",
        "wordcloud = WordCloud(width=1000, height=600,max_font_size = 100)\n",
        "plt.figure(figsize=(15,12))\n",
        "plt.imshow(wordcloud.generate(text), interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2GF-HxRMDDR"
      },
      "outputs": [],
      "source": [
        "#input\n",
        "x= df['cleaned_reviews']\n",
        "#output\n",
        "y = df['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW4fi1-fr_bc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SGQnZa24e37"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYQmAV3ySQg5"
      },
      "outputs": [],
      "source": [
        "#TfidVectorizer --> TF(Term Freq) * IDF (Inverse Doc Freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4uQJxeaTMvT"
      },
      "outputs": [],
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j71Yx9NHqWmh"
      },
      "outputs": [],
      "source": [
        "x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uba2WfLy5u_6"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_BBB3tuzay-"
      },
      "source": [
        "Embedding with Sentence Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4He8HgUck_ll"
      },
      "outputs": [],
      "source": [
        "# Install sentence-transformers library\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "# Import model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load pre_trained model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Make sure all reviews are string type\n",
        "df['cleaned_reviews'] = df['cleaned_reviews'].astype(str)\n",
        "\n",
        "# Convert text to dense embeddings\n",
        "embeddings = model.encode(df['cleaned_reviews'].tolist(), show_progress_bar=True)\n",
        "\n",
        "#label encoding (Prepare target)\n",
        "label_map = {'CG': 0, 'OR': 1}\n",
        "df['label'] = df['label'].map(label_map)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(embeddings, df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression  model\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model performance\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=['CG', 'OR']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3HLpJa0APWJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Display the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['CG (Fake)', 'OR (Real)'])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix for Fake Review Detection\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.savefig(\"confusion_matrix.png\", dpi=300)\n"
      ],
      "metadata": {
        "id": "cVoAS-RrYBSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "268Z_vN1AeXx"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"confusion_matrix.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC curve after confusion matrix\n",
        "\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get scores (not hard predictions) for ROC\n",
        "y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Calculate AUC\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='Logistic Regression (AUC = {:.2f})'.format(auc))\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal for baseline\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.savefig(\"roc_curve.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "L611HnUk1Hmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.savefig(\"roc_curve.png\", dpi=300)\n"
      ],
      "metadata": {
        "id": "gmUsZUId3SrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"roc_curve.png\")\n"
      ],
      "metadata": {
        "id": "ox7s7ULt3WbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Get classification report as a dictionary\n",
        "report = classification_report(y_test, y_pred, target_names=['CG (Fake)', 'OR (Real)'], output_dict=True)\n",
        "\n",
        "# Convert to DataFrame for plotting\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "\n",
        "# Only keep classes, not 'accuracy', 'macro avg', etc.\n",
        "df_report = df_report.iloc[:2][['precision', 'recall', 'f1-score']]\n",
        "\n",
        "# Plot\n",
        "df_report.plot(kind='bar', figsize=(8, 6))\n",
        "plt.title('Precision, Recall, and F1-Score per Class')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"f1_scores_barplot.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "K5PftKycYSZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"f1_scores_barplot.png\")\n"
      ],
      "metadata": {
        "id": "8hL9EWnEYXR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1klr6DjJMMP"
      },
      "outputs": [],
      "source": [
        "#install a python library for creating simple web apps\n",
        "!pip install -q --upgrade gradio\n",
        "\n",
        "#import gradio for building interface\n",
        "import gradio as gr\n",
        "\n",
        "#define function\n",
        "def review_checker(Review):\n",
        "    emb = model.encode([Review])# Convert input review to sentence embedding\n",
        "    pred = clf.predict(emb)[0]  # Predict label using trained model\n",
        "    return \"Fake\" if pred == 1 else \"Real\" # return output\n",
        "\n",
        "gr.Interface(fn=review_checker, inputs=\"textbox\", outputs=\"label\", title=\"Fake Review Detector\").launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbstripout\n",
        "!nbstripout MinorProject.ipynb\n"
      ],
      "metadata": {
        "id": "0kIss0j-MJSe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}